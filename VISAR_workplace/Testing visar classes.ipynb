{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepchem DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T06:54:02.587749Z",
     "start_time": "2020-02-18T06:53:59.606373Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "RDKit WARNING: [14:54:00] Enabling RDKit 2019.09.1 jupyter extensions\n",
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from visar.deepchem_utils import prepare_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T06:24:59.673864Z",
     "start_time": "2020-02-18T06:24:59.667897Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict_visar = {\n",
    "    'model_name': 'baseline_reg',\n",
    "    'task_list': ['T107'],\n",
    "    'eval_type': 'regression',\n",
    "    # input data related params:\n",
    "    'dataset_file': '../data/MT_data_clean_June28.csv',\n",
    "    'feature_type': 'Circular_2048',\n",
    "    'id_field': 'molregno',\n",
    "    'smiles_field': 'salt_removed_smi',\n",
    "    'model_flag': 'MT',\n",
    "    'add_features': None,\n",
    "    'frac_train': 0.8,\n",
    "    'rand_seed': 0,\n",
    "    # model architecture related parameters:\n",
    "    'baseline_type': 'RidgeCV'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T06:25:21.981738Z",
     "start_time": "2020-02-18T06:25:02.041784Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader, test_loader, train_df, test_df = prepare_dataset(para_dict_visar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T06:25:33.046921Z",
     "start_time": "2020-02-18T06:25:32.818377Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader.X.shape, train_loader.y.shape, train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T06:27:30.249914Z",
     "start_time": "2020-02-18T06:27:30.238781Z"
    }
   },
   "outputs": [],
   "source": [
    "from visar.visar_utils import update_bicluster\n",
    "from visar.VISAR_model import visar_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T06:29:02.444946Z",
     "start_time": "2020-02-18T06:28:57.716142Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseline_model = visar_model(para_dict_visar)\n",
    "baseline_model.model_init()\n",
    "baseline_model.fit(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T06:29:55.281516Z",
     "start_time": "2020-02-18T06:29:55.275722Z"
    }
   },
   "outputs": [],
   "source": [
    "print(baseline_model.model.coef_[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T06:31:22.058799Z",
     "start_time": "2020-02-18T06:31:04.773651Z"
    }
   },
   "outputs": [],
   "source": [
    "baseline_model.generate_viz_results(train_loader, train_df, '/ridgeCV_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## deepchem class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T06:54:08.127865Z",
     "start_time": "2020-02-18T06:54:08.119185Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict_DC_robustMT = {\n",
    "    'model_name': 'DC_RobustMT_reg',\n",
    "    'task_list': ['T107', 'T108'],\n",
    "    'eval_type': 'regression',\n",
    "    # input data related params:\n",
    "    'dataset_file': '../data/MT_data_clean_June28.csv',\n",
    "    'feature_type': 'Circular_2048',\n",
    "    'id_field': 'molregno',\n",
    "    'smiles_field': 'salt_removed_smi',\n",
    "    'model_flag': 'MT',\n",
    "    'add_features': None,\n",
    "    'frac_train': 0.8,\n",
    "    'rand_seed': 0,\n",
    "    # model architecture related parameters:\n",
    "    'layer_sizes': [128, 64],\n",
    "    'bypass_layer_sizes': [64],\n",
    "    'dropouts': 0.5,\n",
    "    'bypass_dropouts': 0.5,\n",
    "    # model training related parameters:\n",
    "    'learning_rate': 0.001,\n",
    "    'GPU': False,\n",
    "    'epoch': 40, # training epoch of each round (saving model at the end of each round)\n",
    "    'epoch_num': 20, # how many rounds\n",
    "    # viz file processing related parameters:\n",
    "    'valid_cutoff': None, \n",
    "    'n_layer': 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T06:54:31.860282Z",
     "start_time": "2020-02-18T06:54:10.366726Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3249: DtypeWarning: Columns (767) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted dataset shape: (3471, 4)\n",
      "Loading raw samples now.\n",
      "shard_size: 8192\n",
      "About to start loading CSV from tmp.csv\n",
      "Loading shard 1 of size 8192.\n",
      "Featurizing sample 0\n",
      "Featurizing sample 1000\n",
      "Featurizing sample 2000\n",
      "Featurizing sample 3000\n",
      "TIMING: featurizing shard 0 took 14.182 s\n",
      "TIMING: dataset construction took 14.399 s\n",
      "Loading dataset from disk.\n",
      "Computing train/valid/test indices\n",
      "TIMING: dataset construction took 0.339 s\n",
      "Loading dataset from disk.\n",
      "TIMING: dataset construction took 0.184 s\n",
      "Loading dataset from disk.\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader, train_df, test_df = prepare_dataset(para_dict_DC_robustMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:34:11.139139Z",
     "start_time": "2020-02-18T07:34:11.061568Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import deepchem as dc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from visar.deepchem_utils import (\n",
    "    prepare_dataset,\n",
    "    ST_model_layer1,\n",
    "    ST_model_layer2\n",
    "    )\n",
    "\n",
    "from visar.VISAR_model import visar_model\n",
    "from visar.visar_utils import update_bicluster, FP_dim\n",
    "\n",
    "class deepchem_robust_regressor(visar_model):\n",
    "    def __init__(self, para_dict, *args, **kwargs):\n",
    "        super().__init__(para_dict, *args, **kwargs)\n",
    "\n",
    "        # set default parameters\n",
    "\n",
    "        # extract model_related parameters\n",
    "        if self.para_dict['add_features'] is None:\n",
    "            self.n_tasks = len(self.para_dict['task_list'])\n",
    "        else:\n",
    "            self.n_tasks = len(self.para_dict['task_list']) + len(self.para_dict['add_features'])\n",
    "        self.n_features = FP_dim[self.para_dict['feature_type']]\n",
    "        self.layer_sizes = self.para_dict['layer_sizes']\n",
    "        self.bypass_layer_sizes = self.para_dict['bypass_layer_sizes']\n",
    "        self.dropout = self.para_dict['dropouts']\n",
    "        self.bypass_dropouts = self.para_dict['bypass_dropouts']\n",
    "\n",
    "        # get training params\n",
    "        self.lr = self.para_dict['learning_rate']\n",
    "        self.epoch_num = self.para_dict['epoch_num']\n",
    "        self.epoch = self.para_dict['epoch']\n",
    "\n",
    "\n",
    "    def model_init(self):\n",
    "        self.model = dc.models.RobustMultitaskRegressor(n_tasks = self.n_tasks, \n",
    "                                n_features = self.n_features, layer_sizes = self.layer_sizes,\n",
    "                                               bypass_layer_sizes=self.bypass_layer_sizes, \n",
    "                                               bypass_dropouts = self.bypass_dropouts,\n",
    "                                               dropouts = self.dropout, learning_rate = self.lr)\n",
    "        self.model.model_dir = self.save_path\n",
    "        return\n",
    "\n",
    "    def predict(self, data_loader):\n",
    "        return self.model.predict(data_loader)\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        if self.para_dict['eval_type'] == 'regression':\n",
    "            # only mean r2 score for now\n",
    "            metric = dc.metrics.Metric(\n",
    "                dc.metrics.r2_score, np.mean, mode = 'regression')\n",
    "            scores = self.model.evaluate(data_loader, [metric], [], per_task_metrics=metric)\n",
    "            return scores\n",
    "        \n",
    "        elif self.para_dict['eval_type'] == 'classification':\n",
    "            pass\n",
    "\n",
    "    def fit(self, train_loader, test_loader):\n",
    "        train_evaluation = [train_loader.get_task_names()]\n",
    "        test_evaluation = [train_loader.get_task_names()]\n",
    "        for iteration in range(self.epoch_num):\n",
    "            self.model.fit(train_loader, nb_epoch = self.epoch, max_checkpoints_to_keep = 1, checkpoint_interval=20)\n",
    "            print('======== Iteration %d ======' % iteration)\n",
    "            print(\"Evaluating model\")\n",
    "            train_scores = self.evaluate(train_loader)\n",
    "            train_evaluation.append(train_scores[1][\"mean-r2_score\"])\n",
    "            print(\"Training R2 score: %f\" % train_scores[0][\"mean-r2_score\"])\n",
    "            test_scores = self.evaluate(test_loader)\n",
    "            test_evaluation.append(test_scores[1][\"mean-r2_score\"])\n",
    "            print(\"Test R2 score: %f\" % test_scores[0][\"mean-r2_score\"])\n",
    "        \n",
    "            # save evaluation scores\n",
    "            train_df = pd.DataFrame(np.array(train_evaluation))\n",
    "            test_df = pd.DataFrame(np.array(test_evaluation))\n",
    "            train_df.to_csv(self.save_path + '/train_log.csv', index = None)\n",
    "            test_df.to_csv(self.save_path + '/test_log.csv', index = None)\n",
    "\n",
    "    # --------------------------------\n",
    "    def save_param(self, path = None):\n",
    "        if path==None:\n",
    "            filepath = os.path.join(self.model_path, 'train_parameters.json')\n",
    "        else:\n",
    "            filepath = os.path.join(path, 'train_parameters.json')\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.para_dict, f, indent=2)\n",
    "\n",
    "    def load_param(self, path = None):\n",
    "        if path == None:\n",
    "            filepath = os.path.join(self.model_path, 'train_parameters.json')\n",
    "        else:\n",
    "            filepath = os.path.join(path, 'train_parameters.json')\n",
    "        if os.path.exists(filepath):\n",
    "            return json.load(open(filepath, 'r'))\n",
    "        return None\n",
    "\n",
    "    def load_model(self, prev_model):\n",
    "        self.model.restore(checkpoint = prev_model)\n",
    "        return\n",
    "\n",
    "    # --------------------------------\n",
    "    def get_weights_RobustMT(self, layer_variables):\n",
    "        with self.model._get_tf(\"Graph\").as_default():\n",
    "            w1 = model.session.run(layer_variables[0])\n",
    "            b1 = model.session.run(layer_variables[1])\n",
    "        return [w1, b1]\n",
    "\n",
    "    def get_transfer_model(self, n_layer = 2):\n",
    "        # load previous parameters\n",
    "        tot_layer_variables = self.model.get_variables()\n",
    "        param1 = self.get_weights_RobustMT(self.model, [tot_layer_variables[0], tot_layer_variables[1]])\n",
    "        param2 = self.get_weights_RobustMT(self.model, [tot_layer_variables[2], tot_layer_variables[3]])\n",
    "    \n",
    "        n_features = param1[0].shape[0]\n",
    "        layer_size = [param1[0].shape[1], param2[0].shape[1]]\n",
    "    \n",
    "        if n_layer == 1:\n",
    "            transfer_model = ST_model_layer1(self.n_features, layer_size, [param1, param2])\n",
    "        elif n_layer == 2:\n",
    "            transfer_model = ST_model_layer2(self.n_features, layer_size, [param1, param2])\n",
    "        else:\n",
    "            print('invalid layer size!')\n",
    "        return transfer_model\n",
    "\n",
    "    def get_coords(self, transfer_model, train_loader, custom_loader = None, mode = 'default'):\n",
    "        if mode == 'default':\n",
    "            transfer_values = transfer_model.predict(train_loader.X)\n",
    "            N_training = train_loader.X.shape[0]\n",
    "            if not custom_loader is None:\n",
    "                transfer_values2 = transfer_model.predict(custom_loader.X)\n",
    "                N_custom = len(custom_loader.X)\n",
    "                transfer_values = np.concatenate((transfer_values, transfer_values2), axis = 0)\n",
    "\n",
    "            pca = PCA(n_components = 20)\n",
    "            value_reduced_20d = pca.fit_transform(transfer_values)\n",
    "            tsne = TSNE(n_components = 2)\n",
    "            value_reduced = tsne.fit_transform(value_reduced_20d)\n",
    "\n",
    "            if not custom_loader is None:\n",
    "                return value_reduced[0:N_training,:], value_reduced[N_training:(N_training+N_custom),:]\n",
    "            else:\n",
    "                return value_reduced, None\n",
    "\n",
    "    # gradient calculation\n",
    "    def calculate_gradients(self, X_train, task_tensor_name, prev_model):\n",
    "        '''\n",
    "        Calculate the gradients for each chemical\n",
    "        input: X_train --- fingerprint matrix of the chemicals of interest\n",
    "               prev_model -- trained neural network model\n",
    "        output: the gradient matrix\n",
    "        '''\n",
    "        feed_dict = {}\n",
    "\n",
    "        with tf.Graph().as_default():\n",
    "            with tf.Session() as sess:\n",
    "                K.set_session(sess)\n",
    "\n",
    "                new_saver = tf.train.import_meta_graph(prev_model + '.meta')\n",
    "                new_saver.restore(sess, prev_model)\n",
    "                graph = tf.get_default_graph()\n",
    "\n",
    "                feed_dict['Feature_8/PlaceholderWithDefault:0'] = X_train\n",
    "                #feed_dict['Dense_7/Dense_7/Relu:0'] = X_train[0:10,0:512]\n",
    "                feed_dict['Placeholder:0'] = 1.0\n",
    "\n",
    "                op_tensor = graph.get_tensor_by_name(task_tensor_name)\n",
    "                X = graph.get_tensor_by_name('Feature_8/PlaceholderWithDefault:0')\n",
    "                #X = graph.get_tensor_by_name('Dense_7/Dense_7/Relu:0')\n",
    "\n",
    "                reconstruct = tf.gradients(op_tensor, X)[0]\n",
    "                out = sess.run(reconstruct, feed_dict = feed_dict)[0]\n",
    "\n",
    "        K.clear_session()\n",
    "        return out\n",
    "\n",
    "    def generate_task_df(self, dataset, prev_model, valid_mask):\n",
    "        n_bypass = len(self.bypass_layer_sizes)\n",
    "        TASK_LAYERS = ['Dense_%d/Dense_%d/Relu:0' % (10 + n_bypass * 2 * idx, 10 + n_bypass * 2 * idx)\n",
    "                        for idx in range(n_tasks)]\n",
    "        TASK_LAYERS = list(np.array(TASK_LAYERS)[valid_mask])\n",
    "        SHARE_LAYER = 'Dense_7/Dense_7/Relu:0'\n",
    "        grad_mat = np.zeros((len(TASK_LAYERS)+1, self.n_features))\n",
    "\n",
    "        for i in range(len(TASK_LAYERS)):\n",
    "            grad_mat[i,:] = calculate_gradients(dataset.X, TASK_LAYERS[i], prev_model)\n",
    "        grad_mat[len(TASK_LAYERS),:] = calculate_gradients(dataset.X, SHARE_LAYER, prev_model)\n",
    "        self.task_df = pd.DataFrame(grad_mat.T)\n",
    "        self.task_df.columns = list(self.tasks[valid_mask]) + ['SHARE']\n",
    "\n",
    "        return\n",
    "        \n",
    "    def generate_viz_results(self, train_loader, train_df, output_prefix,\n",
    "                             custom_loader = None, custom_df = None, prev_model = None):\n",
    "        self.load_model(self, prev_model)\n",
    "\n",
    "        # get the actual task list from log files\n",
    "        test_log_df = pd.read_csv(self.save_path + 'test_log.csv')\n",
    "        self.tasks = test_log_df.columns.values\n",
    "\n",
    "        if self.para_dict['valid_cutoff'] is not None:\n",
    "            final_merit = test_log_df.iloc[-1,].values\n",
    "            valid_mask = final_merit > valid_cutoff\n",
    "        else:\n",
    "            valid_mask = np.array([True] * self.n_tasks)\n",
    "\n",
    "        print('------------- Prepare information for chemicals ------------------')\n",
    "        # calculate transfer values and coordinates\n",
    "        model_transfer = self.get_transfer_model(n_layer = self.para_dict['n_layer'])\n",
    "        coord_values1, coord_values2 = self.get_coords(model_transfer, train_loader, custom_loader)\n",
    "\n",
    "        # prediction for the training set\n",
    "        self.compound_df1 = generate_compound_df(train_loader, train_df, coord_values1, valid_mask)\n",
    "        if not custom_loader is None:\n",
    "            self.compound_df2 = generate_compound_df(custom_loader, custom_df, coord_values2, valid_mask)\n",
    "\n",
    "        print('------------- Prepare information for minibatches ------------------')\n",
    "        # clustering\n",
    "        self.generate_batch_df(train_loader, custom_loader, coord_values1, coord_values2)\n",
    "\n",
    "        print('------------- Prepare information for tasks ------------------')\n",
    "        # derivative/gradient/sensitivity calculation\n",
    "        self.generate_task_df(train_loader, prev_model, valid_mask)\n",
    "\n",
    "        print('------- Generate color labels with default K of 5 --------')\n",
    "        # color mapping\n",
    "        batch_df, task_df, compound_df = update_bicluster(self.batch_df, self.task_df, self.compound_df1, mode = 'RobustMT', K = 5)\n",
    "        if not custom_loader is None:\n",
    "            lut2 = dict(zip(batch_df['Label_id'], batch_df['batch_label_color']))\n",
    "            lut22 = dict(zip(batch_df['Label_id'], batch_df['batch_label']))\n",
    "            lut222 = dict(zip(compound_df['label'], compound_df['label_color']))\n",
    "            compound_df2['batch_label_color'] = self.compound_df2['label'].map(lut2)\n",
    "            compound_df2['batch_label'] = self.compound_df2['label'].map(lut22)\n",
    "            compound_df2['label_color'] = self.compound_df2['label'].map(lut222)\n",
    "\n",
    "        print('-------------- Saving datasets ----------------')\n",
    "        # saving results\n",
    "        compound_df.to_csv(self.save_path + output_prefix + 'compound_df.csv', index = False)\n",
    "        batch_df.to_csv(self.save_path + output_prefix + 'batch_df.csv', index = False)\n",
    "        task_df.to_csv(self.save_path + output_prefix + 'task_df.csv', index = False)\n",
    "\n",
    "        if not custom_loader is None:\n",
    "            compound_df2.to_csv(output_prefix + 'compound_custom_df.csv', index = False)\n",
    "        \n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:06:23.603402Z",
     "start_time": "2020-02-18T07:06:23.594770Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class deepchem_robust_regressor in module __main__:\n",
      "\n",
      "class deepchem_robust_regressor(visar.VISAR_model.visar_model)\n",
      " |  Method resolution order:\n",
      " |      deepchem_robust_regressor\n",
      " |      visar.VISAR_model.visar_model\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, para_dict, *args, **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  calculate_gradients(self, X_train, task_tensor_name, prev_model)\n",
      " |      Calculate the gradients for each chemical\n",
      " |      input: X_train --- fingerprint matrix of the chemicals of interest\n",
      " |             prev_model -- trained neural network model\n",
      " |      output: the gradient matrix\n",
      " |  \n",
      " |  evaluate(self, data_loader)\n",
      " |  \n",
      " |  fit(self, dataset)\n",
      " |  \n",
      " |  generate_task_df(self, dataset, prev_model, valid_mask)\n",
      " |  \n",
      " |  generate_viz_results(self, train_loader, train_df, output_prefix, custom_loader=None, custom_df=None, prev_model=None)\n",
      " |  \n",
      " |  get_coords(self, transfer_model, train_loader, custom_loader=None, mode='default')\n",
      " |  \n",
      " |  get_transfer_model(self, n_layer=2)\n",
      " |  \n",
      " |  get_weights_RobustMT(self, layer_variables)\n",
      " |      # --------------------------------\n",
      " |  \n",
      " |  load_model(self, prev_model)\n",
      " |  \n",
      " |  load_param(self, path=None)\n",
      " |  \n",
      " |  model_init(self)\n",
      " |  \n",
      " |  predict(self, data_loader)\n",
      " |  \n",
      " |  save_param(self, path=None)\n",
      " |      # --------------------------------\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from visar.VISAR_model.visar_model:\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  cluster_MiniBatch(self, values, grain_size=30)\n",
      " |  \n",
      " |  generate_batch_df(self, train_loader, custom_loader, coord_values1, coord_values2)\n",
      " |  \n",
      " |  generate_compound_df(self, data_loader, df, coord_values, id_field, valid_mask=None)\n",
      " |  \n",
      " |  generate_instance_analysis(self, smiles_string, prev_model, valid_mask, pos_cut=3, neg_cut=-3, nBits=2048)\n",
      " |      map the gradient of Morgan fingerprint bit on the molecule\n",
      " |      Input:\n",
      " |          smi - the smiles of the molecule (a string)\n",
      " |          gradient - the 2048 coeffients of the feature\n",
      " |          cutoff - if positive, get the pos where the integrated weight is bigger than the cutoff;\n",
      " |                  if negative, get the pos where the integrated weight is smaller than the cutoff\n",
      " |      Output:\n",
      " |          two list of atom ids (positive and negative)\n",
      " |  \n",
      " |  save_model(self)\n",
      " |      # --------------------------------\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from visar.VISAR_model.visar_model:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(deepchem_robust_regressor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:34:18.980231Z",
     "start_time": "2020-02-18T07:34:18.975370Z"
    }
   },
   "outputs": [],
   "source": [
    "robust_model = deepchem_robust_regressor(para_dict_DC_robustMT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:34:20.940493Z",
     "start_time": "2020-02-18T07:34:20.710423Z"
    }
   },
   "outputs": [],
   "source": [
    "robust_model.model_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:16:30.752215Z",
     "start_time": "2020-02-18T07:16:30.746213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RobustMultitaskRegressor(activation_fns=None, bias_init_consts=None,\n",
      "                         bypass_bias_init_consts=None, bypass_dropouts=None,\n",
      "                         bypass_layer_sizes=None,\n",
      "                         bypass_weight_init_stddevs=None, dropouts=None,\n",
      "                         layer_sizes=None, n_features=2048, n_tasks=2,\n",
      "                         weight_decay_penalty=None,\n",
      "                         weight_decay_penalty_type=None,\n",
      "                         weight_init_stddevs=None)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(robust_model.__repr__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:34:25.784250Z",
     "start_time": "2020-02-18T07:34:24.295441Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = robust_model.predict(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:38:07.179892Z",
     "start_time": "2020-02-18T07:34:27.050617Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Iteration 0 ======\n",
      "Evaluating model\n",
      "computed_metrics: [0.9428589651853301, 0.9419415473750377]\n",
      "Training R2 score: 0.942400\n",
      "computed_metrics: [0.4240126613218471, 0.5108456035531777]\n",
      "Test R2 score: 0.467429\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-f4c2ab06cbbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrobust_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-e8b417b8df93>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, test_loader)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mtest_evaluation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_task_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_checkpoints_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'======== Iteration %d ======'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/deepchem/models/keras_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             deterministic=deterministic), max_checkpoints_to_keep,\n\u001b[0;32m--> 307\u001b[0;31m         checkpoint_interval, restore, variables, loss, callbacks)\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m   def fit_generator(self,\n",
      "\u001b[0;32m/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/deepchem/models/keras_model.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_label_placeholders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_weights_placeholders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0mfetched_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetched_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0mcurrent_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetched_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 950\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    951\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1173\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1350\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1356\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1357\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1339\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1341\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/root/anaconda3/envs/deepchem_visar/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1427\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "robust_model.fit(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:44:27.042454Z",
     "start_time": "2020-02-18T07:44:27.039375Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prev_model = robust_model.model.model_dir + '/ckpt-65'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:44:29.386203Z",
     "start_time": "2020-02-18T07:44:29.018883Z"
    }
   },
   "outputs": [],
   "source": [
    "robust_model.load_model(prev_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:46:16.866803Z",
     "start_time": "2020-02-18T07:46:16.852375Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation_fns': None,\n",
       " 'bias_init_consts': None,\n",
       " 'bypass_bias_init_consts': None,\n",
       " 'bypass_dropouts': None,\n",
       " 'bypass_layer_sizes': None,\n",
       " 'bypass_weight_init_stddevs': None,\n",
       " 'dropouts': None,\n",
       " 'layer_sizes': None,\n",
       " 'n_features': 2048,\n",
       " 'n_tasks': 2,\n",
       " 'weight_decay_penalty': None,\n",
       " 'weight_decay_penalty_type': None,\n",
       " 'weight_init_stddevs': None}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robust_model.model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-18T07:33:16.885974Z",
     "start_time": "2020-02-18T07:33:16.881226Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.is_gpu_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem_visar",
   "language": "python",
   "name": "deepchem_visar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
